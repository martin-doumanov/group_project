---
title: "Final Group Project: AirBnB analytics"
date: "12 Oct 2021"
author: "Reading Time: About 8 minutes"
output:
  html_document:
    highlight: zenburn
    theme: flatly
    toc: yes
    toc_float: yes
    number_sections: yes
    code_folding: show
---


```{r setup, include=FALSE}
# leave this chunk alone
options(knitr.table.format = "html") 
knitr::opts_chunk$set(warning = FALSE, message = FALSE, 
  comment = NA, dpi = 300)
```


```{r load-libraries, echo=FALSE}

library(tidyverse) # the usual stuff: dplyr, readr, and other goodies
library(lubridate) # to handle dates
library(GGally) # for correlation-scatter plot matrix
library(ggfortify) # to produce residual diagnostic plots
library(rsample) # to split dataframe in training- & testing sets
library(janitor) # clean_names()
library(broom) # use broom:augment() to get tidy table with regression output, residuals, etc
library(huxtable) # to get summary table of all models produced
library(kableExtra) # for formatting tables
library(moderndive) # for getting regression tables
library(skimr) # for skim
library(mosaic)
library(leaflet) # for interactive HTML maps
library(tidytext)
library(viridis)
library(vroom)
library(Hmisc) # to compute correlation matrix with p-values
library(corrplot) # to plot corr matrix
```


In your final group assignment you have to analyse data about Airbnb listings and fit a model to predict the total cost for two people staying 4 nights in an AirBnB in a city. You can download AirBnB data from [insideairbnb.com](http://insideairbnb.com/get-the-data.html){target="_blank"}; it was originally scraped from airbnb.com. 

The following [Google sheet](https://docs.google.com/spreadsheets/d/1QrR-0PUGVWvDiVQL4LOk7w-xXwiDnM3dDtW6k15Hc7s/edit?usp=sharing) shows which cities you can use; please choose one of them and add your group name next to it, e.g., A7, B13. No city can have more than 2 groups per stream working on it; if this happens, I will allocate study groups to cities with the help of R's sampling.


All of the listings are a GZ file, namely they are archive files compressed by the standard GNU zip (gzip) compression algorithm. You can download, save and extract the file if you wanted, but `vroom::vroom()` or `readr::read_csv()` can immediately read and extract this kind of a file. You should prefer `vroom()` as it is faster, but if vroom() is limited by a firewall, please use `read_csv()` instead.


`vroom` will download the *.gz zipped file, unzip, and provide you with the dataframe. 


```{r load_data, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}

# use cache=TRUE so you dont donwload the data everytime you knit

listings <- vroom("http://data.insideairbnb.com/germany/be/berlin/2021-09-21/data/listings.csv.gz") %>% 
       clean_names()

```


Even though there are many variables in the dataframe, here is a quick description of some of the variables collected, and you can find a [data dictionary here](https://docs.google.com/spreadsheets/d/1iWCNJcSutYqpULSQHlNyGInUvHg2BoUGoNRIGa6Szc4/edit#gid=982310896)

- `price` = cost per night 
- `property_type`: type of accommodation (House, Apartment, etc.)
- `room_type`:

  - Entire home/apt (guests have entire place to themselves)
  - Private room (Guests have private room to sleep, all other rooms shared)
  - Shared room (Guests sleep in room shared with others)

- `number_of_reviews`: Total number of reviews for the listing
- `review_scores_rating`: Average review score (0 - 100)
- `longitude` , `latitude`: geographical coordinates to help us locate the listing
- `neighbourhood*`: three variables on a few major neighbourhoods in each city 


# Exploratory Data Analysis (EDA)

In the [R4DS Exploratory Data Analysis chapter](http://r4ds.had.co.nz/exploratory-data-analysis.html){target="_blank"}, the authors state:

> "Your goal during EDA is to develop an understanding of your data. The easiest way to do this is to use questions as tools to guide your investigation... EDA is fundamentally a creative process. And like most creative processes, the key to asking quality questions is to generate a large quantity of questions."


Conduct a thorough EDA. Recall that an EDA involves three things:

* Looking at the raw values.
    * `dplyr::glimpse()`
    
```{r}
listings %>% 
  glimpse()
```
> Our dataset has 18,288 observations and 74 variables. It includes details about Airbnb listings in Berlin.

### Computing summary statistics of the variables of interest, or finding NAs

```{r}

listings %>% #skimming only the training data, not test to avoid cheating
  skim()

```
> Using the skim function, we can see that we have 25 character variables, 5 date variables, 7 logical variables and 37 numeric variables. While many variables have no missing values, some variables like 'host_about', 'host_neighbourhood', 'neighbourhood' and 'license' have many missing observations. In fact, 'bathrooms' and 'calendar_update' have no observations at all.

We will now list the variable names by type:

### Numeric
```{r}
listings %>% 
  skim() %>% 
  filter(skim_type=="numeric") %>% 
  select(skim_variable)
```
### Factor
```{r}
listings %>% 
  skim() %>% 
  filter(skim_type=="logical") %>% 
  select(skim_variable)
```
### Character
```{r}
listings %>% 
  skim() %>% 
  filter(skim_type=="character") %>% 
  select(skim_variable)
```
Notice that the variables 'host_response_rate', 'host_acceptance_rate' and 'price' have been categorized as character strings instead of numeric variables.

Since `price` is a quantitative variable, we need to make sure it is stored as numeric data `num` in the dataframe. To do so, we will first use `readr::parse_number()` which drops any non-numeric characters before or after the first number. We will also convert `host_response_rate` and `host_acceptance_rate`.

```{r}
listings <- listings %>% 
  mutate(price = parse_number(price),
         host_response_rate = parse_number(host_response_rate)/100,
         host_acceptance_rate = parse_number(host_acceptance_rate)/100
         )
# We check the new type of the three variables ...
typeof(listings$price)
typeof(listings$host_response_rate)
typeof(listings$host_acceptance_rate) #Price, host response rate and host acceptance rate are now stored as numbers for the training set

# ... and take a closer look at the variables
listings %>% 
  select(price, host_response_rate, host_acceptance_rate) %>% 
  glimpse()

```

Now let's look at the rest of the data in more detail. We will start by looking at the variable property type.

## Property types

We look at the variable `property_type` and use the `count` function to determine how many categories there are, their frequency and their proportions. 

```{r}
listings %>% 
  count(property_type, sort=TRUE) %>% 
  count() #There are 64 categories.
  
property_counts <- listings %>% 
  count(property_type, sort=TRUE) %>%
  mutate(sum_n=sum(n)) %>% 
  slice_max(order_by=n, n=4) %>% 
  mutate(prop=n/sum_n)

property_counts

property_counts %>% 
  summarise(total_prop = sum(prop))
  
```
We can see that there are 64 different property types. The top 4 most common property types are entire rental units (48.3%), private rooms in rental units (35.6%), entire condos (2.7%) & entire serviced apartments (2.0%), which totals 88.5% of the data.


Since the vast majority of the observations in the data are one of the top four or five property types, we would like to create a simplified version of `property_type` variable that has 5 categories: the top four categories and `Other`. Fill in the code below to create `prop_type_simplified`.

```{r}
listings <- listings %>%
  mutate(prop_type_simplified = case_when(
    property_type %in% c("Entire rental unit","Private room in rental unit", "Entire condominium (condo)","Entire serviced apartment") ~ property_type, 
    TRUE ~ "Other"
  ))
  
```
We can use the code below to check that `prop_type_simplified` was correctly made.

```{r}
listings %>%
  count(property_type, prop_type_simplified) %>%
  arrange(desc(n))
```
# ADD A BAR OR COL GRAPH HERE

## Prices - Creating the Dependent Variable

First let's look at "price" in more detail:
```{r}

favstats(~price, data = listings)

```

We can see that none of the price observations are missing, however the minimum price per night is 0, which seems suspicious. We will investigate this further and see how many observations have a price of 0.

```{r}
listings %>% 
  filter(price==0) %>% 
  count()
```
There are 8 observations where price takes the value 0.

Next we are going to create a new variable price_4_nights, which represents the price we would have to pay to stay at an Airbnb property for 4 nights. Since we are interested in rentiong for two, we will exclude any properties that accommodate less than 2 people. We also exclude any properties with negative or 0 prices.

```{r}
listings <- listings %>% 
  filter(accommodates >= 2) %>%
  filter(price > 0) %>% 
  mutate(price_4_nights = 4*price)

```

It is important to know the distribution of our dependent variable. To check this, we will plot a histogram of price_4_nights with the code below:

```{r}
listings %>% 
  ggplot(aes(x = price_4_nights)) +
  geom_histogram(color="grey") +
  labs(title = "Distribution of Prices of Airbnb Appartment in Berlin for 4 nights",
       subtitle = "Data inlcudes only appartments that accomodate at least 2 people",
       x = "Price per 4 nights",
       y = "",
       caption="Data from: http://insideairbnb.com/get-the-data.html")


```
As we can see, the data on prices is heavily right-skewed. This can hurt the interpretative power of our model and we would need to address it before continuing with our regression analysis. Additionally, we see that although most properties lie within a range, there are some outlines very far from the median. This would also need to be adressed below.

To deal with the distribution of prices, we will transform the price_4_nights using a log function.

```{r}
listings <- listings %>% 
  mutate(log_price_4_nights = log(price_4_nights))

listings %>% 
  ggplot(aes(x = log_price_4_nights)) +
  geom_histogram(color="grey") +
  geom_vline(aes(xintercept = median(log_price_4_nights)),
            color="blue", linetype="dashed", size=1)+
  labs(title = "Distribution of Log Prices of Airbnb Appartment in Berlin for 4 nights",
       subtitle = "Data inlcudes only appartments that accomodate at least 2 people",
       x = "Log of Price per 4 nights",
       y = "",
       caption="Data from: http://insideairbnb.com/get-the-data.html")

```
As we can see, applying a log transformation made the distribution of prices much more like a normal distribution, although there is still a right-skew.

#Add correlation data


## Mimimum Nights

Airbnb is most commonly used for travel purposes, i.e., as an alternative to traditional hotels. However, some properties might be intended for other purposes. We only want to include  listings in our regression analysis that are intended for travel purposes.That is why we need to look at minimum_nights in more detail:

```{r}
# First, we want to see what are the most common values for minimum_nights
listings %>%
  count(minimum_nights) %>%
  arrange(desc(n))

# We also look at the range of values for minimum_bights 
favstats(~minimum_nights, data = listings)

```

As we predicted, most properties on Airbnb seem to be meant for travelers. The most common values are 2,1 and 3 nights, which would be reasonable for a tourist. We can also see, however, that some that have a 30 and 60 day minimum stay are among the 10 most common type of properties. We can guess these properties are meant as a substitute to renting from an agent or a landlord. In other words, these properties are meant for long-term stay and are not appropriate for our analysis. additionally, we can see that the median minimum stay requirement is 3 nights, where as the mean is around 9. The discrepancy comes from the large outliers we see in the data. For example, one property has a minimum stay of 1,124 nights, which heavily skews our data. 

To deal with this, we will filter the data to include only properties that have a minimum stay of less than 5 night.

```{r}
listings <- listings %>% 
  filter(minimum_nights <= 4)

favstats(~minimum_nights, data = listings)

listings %>% 
  ggplot(aes(x = minimum_nights)) +
  geom_histogram(color="grey") +
  labs(title = "Distribution of Minimum Stay Requirements of Airbnb Apparments in Berlin",
       subtitle = "Data inlcudes only appartments that accomodate at least 2 people",
       x = "Minimum Nights Stay Requirement",
       y = "",
       caption="Data from: http://insideairbnb.com/get-the-data.html")
```


#correlation data not linear so take logs



    * `mosaic::favstats()`
* Creating informative visualizations.
    * `ggplot2::ggplot()`
        * `geom_histogram()` or `geom_density()` for numeric continuous variables
        * `geom_bar()` or `geom_col()` for categorical variables
    * `GGally::ggpairs()` for scaterplot/correlation matrix
        * Note that you can add transparency to points/density plots in the `aes` call, for example: `aes(colour = gender, alpha = 0.4)`
        
You may wish to have a level 1 header (`#`) for your EDA, then use level 2 sub-headers (`##`) to make sure you cover all three EDA bases. **At a minimum** you should address these questions:



- What are the correlations between variables? Does each scatterplot support a linear relationship between variables? Do any of the correlations appear to be conditional on the value of a categorical variable?

At this stage, you may also find you want to use `filter`, `mutate`, `arrange`, `select`, or `count`. Let your questions lead you! 

> In all cases, please think about the message your plot is conveying. Don’t just say "This is my X-axis, this is my Y-axis", but rather what’s the **so what** of the plot. Tell some sort of story and speculate about the differences in the patterns in no more than a paragraph.


## Data wrangling


## USE LOGARITHM OF PRICE BECAUSE IT IS HEAVILY SKEWED. WE ALSO NEED TO REMOVE EXTREME VALUES, NEGATIVE PRICES. IF YOU TRANSFORM THE VARIABLES (E.G. BY USING FACTOR LEVELS). EXPECT YOUR ADJUSTED R-SQUARED TO BE BETWEEN 0.1-0.5. IN THE PRESENTATION, DON'T PUT SCREENSHOTS OF THE REGRESSION OUTPUT. SHOW VISUALISATIONS. SHOW CONFIDENCE INTERVALS. WHEN YOU BUILD YOUR MODEL, SPLIT IT INTO TRAINING AND VALIDATION. IN THE SLIDES, PEOPLE ONLY CARE ABOUT THE FINAL MODEL.SHOW THE RESULTS; NO ONE CARES ABOUT THE PROBLEM. DON'T SHOW US A HISTOGRAM - USE MORE ADVANCED GGPLOTS. FEEL FREE TO CRITICIZE THE MODEL AND GIVE ADVICE ABOUT WHAT DATA WOULD BE HELPFUL TO IMPROVE THE MODEL.THE ECONOMIST THEME IS HELPFUL. YOU CAN USE KABLE EXTREME TO MAKE TABLES LOOK NICE.YOU MIGHT WANT TO SHOW CONFIDENCE INTERVALS FOR DIFFERENT PROPERTY TYPES

##USEFUL VARIABLES: review score, number of reviews, superhost, accomodate, property type, room type, cancellation policy, bathrooms, bedrooms, neighbourhood.


##Use at least 960 data points for training. Use at least 40 observations per variable.
##Weekly price might fit better than daily price.

DO NOT CHANGE NA TO 0.
YOU MIGHT WANT TO CREATE A VARIABLE FOR GOOD NEIGHBOURHOODS AND BAD NEIGHBOURHOODS.
DON'T INCLUDE HOST_ID AND OTHER IDENTIFIERS IN THE MODEL.

Feel free to use interaction terms.

# Mapping 

Visualisations of feature distributions and their relations are key to understanding a data set, and they can open up new lines of exploration. We will use the following code to show a map of your city, and overlay all Airbnb coordinates to get an overview of the spatial distribution of properties. For this visualization, we use the `leaflet` package, which includes a variety of tools for interactive maps and allows for easy zoom in-out, clicking on a point to get the actual listing, etc.

```{r, out.width = '80%'}

leaflet(data = listings) %>% 
  addProviderTiles("OpenStreetMap.Mapnik") %>% 
  addCircleMarkers(lng = ~longitude, 
                   lat = ~latitude, 
                   radius = 1, 
                   fillColor = "blue", 
                   fillOpacity = 0.4, 
                   popup = ~listing_url,
                   label = ~property_type)
```



## Correlation Analysis

Before we continue with regression analysis, it is important to understand the correlations between our variables. This will help us avoid multicollinearity problems when we construct our model specifications. The following code constructs a correlation matrix between some of the numeric variables we plan to use in our analysis:

```{r}
listings_cormax <- listings %>% 
  select(host_response_rate, 
         host_listings_count,
         bedrooms,
         beds,
         maximum_nights,
         number_of_reviews,
         number_of_reviews_ltm,
         number_of_reviews_l30d,
         review_scores_rating,
         review_scores_accuracy,
         review_scores_cleanliness,
         review_scores_checkin,
         review_scores_communication,
         review_scores_location,
         review_scores_value,
         reviews_per_month)

cormax <- rcorr(as.matrix(na.omit(listings_cormax)))
cormax
```

#CONTINUE FROM HERE

```{r}
cor <- cor(na.omit(listings_cormax))
corrplot(cor)
```

    
# Regression Analysis

For the target variable $Y$, we will use the cost for two people to stay at an Airbnb location for four (4) nights. 

Create a new variable called `price_4_nights` that uses `price`, and `accomodates` to calculate the total cost for two people to stay at the Airbnb property for 4 nights. This is the variable $Y$ we want to explain.

Use histograms or density plots to examine the distributions of `price_4_nights` and `log(price_4_nights)`. Which variable should you use for the regression model? Why?

Fit a regression model called `model1` with the following explanatory variables: `prop_type_simplified`, `number_of_reviews`, and `review_scores_rating`. 

- Interpret the coefficient `review_scores_rating` in terms of `price_4_nights`.
- Interpret the coefficient of `prop_type_simplified` in terms of `price_4_nights`.

We want to determine if `room_type` is a significant predictor of the cost for 4 nights, given everything else in the model. Fit a regression model called model2 that includes all of the explananatory variables in `model1` plus `room_type`. 


# First, we need to divide our data into a test and training subset. This will allows us to test the predictive power of our model using an out-of-sample test
```{r}

#splitting data into training and test sets

set.seed(1234) #setting a random seed to always get the same split 

train_test_split <- initial_split(listings, prop = 0.80) #taking an 80-20 approach (for all the consultants out there)
listings_train <- training(train_test_split)
listings_test <- testing(train_test_split)

```

Now our test dataset has 3658 observations and training data has 14630 observations with 74 variables each.

We will set our test data aside and do our analysis using only the training data

## Further variables/questions to explore on our own

Our dataset has many more variables, so here are some ideas on how you can extend your analysis

1. Are the number of `bathrooms`, `bedrooms`, `beds`, or size of the house (`accomodates`) significant predictors of `price_4_nights`? Or might these be co-linear variables?
1. Do superhosts `(host_is_superhost`) command a pricing premium, after controlling for other variables?
1. Some hosts allow you to immediately book their listing (`instant_bookable == TRUE`), while a non-trivial proportion don't. After controlling for other variables, is `instant_bookable` a significant predictor of `price_4_nights`?
1. For all cities, there are 3 variables that relate to neighbourhoods: `neighbourhood`, `neighbourhood_cleansed`, and `neighbourhood_group_cleansed`. There are typically more than 20 neighbourhoods in each city, and it wouldn't make sense to include them all in your model. Use your city knowledge, or ask someone with city knowledge, and see whether you can group neighbourhoods together so the majority of listings falls in fewer (5-6 max) geographical areas. You would thus need to create a new categorical variabale `neighbourhood_simplified` and determine whether location is a predictor of `price_4_nights`
1. What is the effect of `avalability_30` or `reviews_per_month` on `price_4_nights`, after we control for other variables?


## Diagnostics, collinearity, summary tables

As you keep building your models, it makes sense to:

1. Check the residuals, using `autoplot(model_x)`
1. As you start building models with more explanatory variables, make sure you use `car::vif(model_x)`` to calculate the **Variance Inflation Factor (VIF)** for your predictors and determine whether you have colinear variables. A general guideline is that a VIF larger than 5 or 10 is large, and your model may suffer from collinearity. Remove the variable in question and run your model again without it.



1. Create a summary table, using `huxtable` (https://mfa2022.netlify.app/example/modelling_side_by_side_tables/) that shows which models you worked on, which predictors are significant, the adjusted $R^2$, and the Residual Standard Error.
1. Finally, you must use the best model you came up with for prediction. Suppose you are planning to visit the city you have been assigned to over reading week, and you want to stay in an Airbnb. Find Airbnb's in your destination city that are apartments with a private room, have at least 10 reviews, and an average rating of at least 90. Use your best model to predict the total cost to stay at this Airbnb for 4 nights. Include the appropriate 95% interval with your prediction. Report the point prediction and interval in terms of `price_4_nights`. 
  - if you used a log(price_4_nights) model, make sure you anti-log to convert the value in $. You can read more about [hot to interpret a regression model when some variables are log transformed here](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faqhow-do-i-interpret-a-regression-model-when-some-variables-are-log-transformed/)


# Deliverables


- By midnight on Monday 18 Oct 2021, you must upload on Canvas a short presentation (max 4-5 slides) with your findings, as some groups will be asked to present in class. You should present your Exploratory Data Analysis, as well as your best model. In addition, you must upload on Canvas your final report, written  using R Markdown to introduce, frame, and describe your story and findings. You should include the following in the memo:

1. Executive Summary: Based on your best model, indicate the factors that influence `price_4_nights`.
This should be written for an intelligent but non-technical audience. All
other sections can include technical writing.
2. Data Exploration and Feature Selection: Present key elements of the data, including tables and
graphs that help the reader understand the important variables in the dataset. Describe how the
data was cleaned and prepared, including feature selection, transformations, interactions, and
other approaches you considered.
3. Model Selection and Validation: Describe the model fitting and validation process used. State
the model you selected and why they are preferable to other choices.
4. Findings and Recommendations: Interpret the results of the selected model and discuss
additional steps that might improve the analysis
  
  

Remember to follow R Markdown etiquette rules and style; don't have the Rmd output extraneous messages or warnings, include summary tables in nice tables (use `kableExtra`), and remove any placeholder texts from past Rmd templates; in other words, (i.e. I don't want to see stuff I wrote in your final report.)
  
  
# Rubric

Your work will be assessed on a rubric which you can find here


```{r rubric, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "rubric.png"), error = FALSE)
```


# Acknowledgements

- The data for this project is from [insideairbnb.com](insideairbnb.com)