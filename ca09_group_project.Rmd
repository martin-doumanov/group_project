---
title: "Final Group Project: AirBnB analytics"
date: "12 Oct 2021"
author: "Reading Time: About 8 minutes"
output:
  html_document:
    highlight: zenburn
    theme: flatly
    toc: yes
    toc_float: yes
    number_sections: yes
    code_folding: show
---


```{r setup, include=FALSE}
# leave this chunk alone
options(knitr.table.format = "html") 
knitr::opts_chunk$set(warning = FALSE, message = FALSE, 
  comment = NA, dpi = 300)

```


```{r load-libraries, echo=FALSE}

library(tidyverse) # the usual stuff: dplyr, readr, and other goodies
library(lubridate) # to handle dates
library(GGally) # for correlation-scatter plot matrix
library(ggfortify) # to produce residual diagnostic plots
library(rsample) # to split dataframe in training- & testing sets
library(janitor) # clean_names()
library(broom) # use broom:augment() to get tidy table with regression output, residuals, etc
library(huxtable) # to get summary table of all models produced
library(kableExtra) # for formatting tables
library(moderndive) # for getting regression tables
library(skimr) # for skim
library(mosaic)
library(leaflet) # for interactive HTML maps
library(tidytext)
library(viridis)
library(vroom)
library(Hmisc) # to compute correlation matrix with p-values
library(corrplot) # to plot corr matrix
library(huxtable)
library(ggthemes)
```


In your final group assignment you have to analyse data about Airbnb listings and fit a model to predict the total cost for two people staying 4 nights in an AirBnB in a city. You can download AirBnB data from [insideairbnb.com](http://insideairbnb.com/get-the-data.html){target="_blank"}; it was originally scraped from airbnb.com. 

The following [Google sheet](https://docs.google.com/spreadsheets/d/1QrR-0PUGVWvDiVQL4LOk7w-xXwiDnM3dDtW6k15Hc7s/edit?usp=sharing) shows which cities you can use; please choose one of them and add your group name next to it, e.g., A7, B13. No city can have more than 2 groups per stream working on it; if this happens, I will allocate study groups to cities with the help of R's sampling.


All of the listings are a GZ file, namely they are archive files compressed by the standard GNU zip (gzip) compression algorithm. You can download, save and extract the file if you wanted, but `vroom::vroom()` or `readr::read_csv()` can immediately read and extract this kind of a file. You should prefer `vroom()` as it is faster, but if vroom() is limited by a firewall, please use `read_csv()` instead.


`vroom` will download the *.gz zipped file, unzip, and provide you with the dataframe. 


```{r load_data, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}

# use cache=TRUE so you dont donwload the data everytime you knit

listings <- vroom("http://data.insideairbnb.com/germany/be/berlin/2021-09-21/data/listings.csv.gz") %>% 
       clean_names()

```


Even though there are many variables in the dataframe, here is a quick description of some of the variables collected, and you can find a [data dictionary here](https://docs.google.com/spreadsheets/d/1iWCNJcSutYqpULSQHlNyGInUvHg2BoUGoNRIGa6Szc4/edit#gid=982310896)

- `price` = cost per night 
- `property_type`: type of accommodation (House, Apartment, etc.)
- `room_type`:

  - Entire home/apt (guests have entire place to themselves)
  - Private room (Guests have private room to sleep, all other rooms shared)
  - Shared room (Guests sleep in room shared with others)

- `number_of_reviews`: Total number of reviews for the listing
- `review_scores_rating`: Average review score (0 - 100)
- `longitude` , `latitude`: geographical coordinates to help us locate the listing
- `neighbourhood*`: three variables on a few major neighbourhoods in each city 


# Exploratory Data Analysis (EDA)


We start by looking at the raw values.
    
```{r}
listings %>% 
  glimpse()
```
> Our dataset has 18,288 observations and 74 variables. It includes details about Airbnb listings in Berlin.

### Computing summary statistics of the variables of interest, or finding NAs

```{r}

listings %>% 
  skim()

```
> Using the skim function, we can see that we have 25 character variables, 5 date variables, 7 logical variables and 37 numeric variables. While many variables have no missing values, some variables like 'host_about', 'host_neighbourhood', 'neighbourhood' and 'license' have many missing observations. In fact, 'bathrooms' and 'calendar_update' have no observations at all.

We will now list the variable names by type:

### Numeric
```{r}
listings %>% 
  skim() %>% 
  filter(skim_type=="numeric") %>% 
  select(skim_variable)
```
### Factor
```{r}
listings %>% 
  skim() %>% 
  filter(skim_type=="logical") %>% 
  select(skim_variable)
```
### Character
```{r}
listings %>% 
  skim() %>% 
  filter(skim_type=="character") %>% 
  select(skim_variable)
```
Notice that the variables 'host_response_rate', 'host_acceptance_rate' and 'price' have been categorized as character strings instead of numeric variables.

Since `price` is a quantitative variable, we need to make sure it is stored as numeric data `num` in the dataframe. To do so, we will first use `readr::parse_number()` which drops any non-numeric characters before or after the first number. We will also convert `host_response_rate` and `host_acceptance_rate`.

```{r}
listings <- listings %>% 
  mutate(price = parse_number(price),
         host_response_rate = parse_number(host_response_rate)/100,
         host_acceptance_rate = parse_number(host_acceptance_rate)/100
         )
# We check the new type of the three variables ...
typeof(listings$price)
typeof(listings$host_response_rate)
typeof(listings$host_acceptance_rate) #Price, host response rate and host acceptance rate are now stored as numbers for the training set

# ... and take a closer look at the variables
listings %>% 
  select(price, host_response_rate, host_acceptance_rate) %>% 
  glimpse()

```
## The variables

Now we will look at the variables in more detail and construct our dependent variable of interest: price per 4 nights for 2 people. 

## Minimum Nights

Airbnb is most commonly used for travel purposes, i.e., as an alternative to traditional hotels. However, some properties might be intended for other purposes. We only want to include  listings in our regression analysis that are intended for travel purposes. In order to do this, we need to look at minimum_nights in more detail:

```{r}
# First, we want to see what are the most common values for minimum_nights
listings %>%
  count(minimum_nights) %>%
  arrange(desc(n))

# We also look at the range of values for minimum_nights 
favstats(~minimum_nights, data = listings)

```

As we predicted, most properties on Airbnb seem to be meant for travelers. The most common values are 2,1 and 3 nights, which would be reasonable for a tourist. We can also see, however, that some that have a 30 and 60 day minimum stay are among the 10 most common type of properties. We can guess these properties are meant as a substitute to renting from an agent or a landlord. In other words, these properties are meant for long-term stay and are not appropriate for our analysis. Additionally, we can see that the median minimum stay requirement is 3 nights, where as the mean is around 9. The discrepancy comes from the large outliers we see in the data. For example, one property has a minimum stay of 1,124 nights, which heavily skews our data. 

To deal with this, we will filter the data to include only properties that have a minimum stay of 4 nights or less.

```{r}
listings <- listings %>% 
  filter(minimum_nights <= 4)

favstats(~minimum_nights, data = listings)

listings %>% 
  ggplot(aes(x = minimum_nights)) +
  geom_histogram(fill = "light blue",
                 colour = "grey") +
  labs(title = "Distribution of Minimum Stay Requirements",
       x = "Minimum Nights Stay Requirement",
       y = "Count",
       caption="Data from: http://insideairbnb.com/get-the-data.html") +
  theme_bw()
```

### Maximum nights

So far we have filtered the data to include only the listings which offer a stay with a minimum length of 1-4 nights. However, it is possible that these include apartments which offer maximum stays of 1-3 days. We need to remove these listings, keeping only those that have maximum stays of 4 nights or more.

```{r}
listings %>% 
  filter(maximum_nights<=3) %>% 
  count() 

listings <- listings %>% 
  filter(maximum_nights >= 4)

favstats(~maximum_nights, data = listings)

listings %>% 
  ggplot(aes(x = maximum_nights)) +
  geom_histogram(fill = "light blue",
                 colour = "grey") +
  labs(title = "Distribution of Maximum Stay Requirements",
       x = "Maximum Nights Stay Requirement",
       y = "Count",
       caption="Data from: http://insideairbnb.com/get-the-data.html") +
  theme_bw()
```
We have now removed the 256 listings whose stays were too short.


### Accommodates

Since we are considering apartments for 2, we also need to filter the data to include only apartments which accommodate at least 2 people. First let's look at summary statistics of the `accommodates` variable.

```{r}

favstats(~accommodates, data=listings)

```
Slightly strangely, there is an apartment in the dataset which claims to accommodate no one. We will leave this issue for now as we will later filter out apartments to include only those that accommodate 2 or more people.

There is also an apartment in the dataset which claims to accommodate 16 people. It's possible that this is an outlier so we will plot a histogram and boxplot to determine whether this is the case.

```{r}

listings %>% 
  ggplot(aes(x = accommodates)) +
  geom_histogram(fill = "light blue",
                 color = "grey") +
  labs(title = "Distribution of people accommodated per Airbnb Apartment in Berlin",
       subtitle = "Data includes only apartments that accommodate at least 2 people",
       x = "Number of people accommodated",
       y = "Count",
       caption="Data from: http://insideairbnb.com/get-the-data.html") +
  theme_bw()


```
The distribution has a positive skew.


```{r}

listings %>% 
  ggplot(aes(x = accommodates)) +
  geom_boxplot(fill="light blue") +
  labs(title = "Distribution of people accommodated per Airbnb Apartment in Berlin",
       x = "Number of people accommodated",
       caption="Data from: http://insideairbnb.com/get-the-data.html") +
  theme_bw()


```
The boxplot shows that there are several outliers. Let's check how many points are classified as outliers and remove some of them.

```{r, fig.show = "hide"} 
accommodates_outliers<-boxplot(listings$accommodates)$out
length(accommodates_outliers) #There are a fair number of 'outliers'. Let's look at these more closely.

accommodates_outliers %>% table() #There are a fairly large number of apartments which accommodate 8 people. 

#We will only discard apartments which accommodate 9 or more people.
  
listings<-listings %>% 
  mutate(accommodates = if_else(accommodates>=9,NA_real_,accommodates)
  )
```

Since we are interested only in apartments for 2 people, we keep only the listings that accommodate at least 2 people.

```{r}
listings<-listings %>% 
  filter(accommodates>=2)
```



### Price

Now that we have filtered the data, let's look at our dependent variable `price` in more detail:
```{r}

favstats(~price, data = listings)

```

We can see that none of the price observations are missing, however the minimum price per night is 0, which seems suspicious. We will investigate this further and see how many observations have a price of 0.

```{r}
listings %>% 
  filter(price==0) %>% 
  count()
```
There are 2 observations where price takes the value 0.

Next we are going to create a new variable price_4_nights, which represents the price we would have to pay to stay at an Airbnb property for 4 nights.  We also exclude any properties with negative or 0 prices.

```{r}
listings <- listings %>% 
  filter(price > 0) %>%
  mutate(price_4_nights = 4*price)

```

It is important to know the distribution of our dependent variable. To check this, we will plot a histogram of price_4_nights with the code below:

```{r}
listings %>% 
  ggplot(aes(x = price_4_nights)) +
  geom_histogram(fill = "light blue", colour = "grey") +
  labs(title = "Distribution of Prices of Airbnb Apartment in Berlin for 4 nights",
       subtitle = "Data includes only apartments that accommodate at least 2 people",
       x = "Price per 4 nights",
       y="Count",
       caption="Data from: http://insideairbnb.com/get-the-data.html") +
  theme_bw()


```
As we can see, the data on prices is heavily right-skewed. This can hurt the interpretative power of our model and we would need to address it before continuing with our regression analysis. 

```{r}
listings %>% 
  ggplot(aes(x = price_4_nights)) +
  geom_boxplot(color="light blue") +
  labs(title = "Distribution of Prices of Airbnb Apartment in Berlin for 4 nights",
       subtitle = "Data includes only apartments that accommodate at least 2 people",
       caption="Data from: http://insideairbnb.com/get-the-data.html") +
  theme_bw()

```

Additionally, we can see from the boxplot that there are some outliers. This will be addressed below.

To deal with the distribution of prices, we will transform the price_4_nights using a log function.

```{r}
listings <- listings %>% 
  mutate(log_price_4_nights = log(price_4_nights))

listings %>% 
  ggplot(aes(x = log_price_4_nights)) +
  geom_histogram(fill = "light blue",
                colour = "grey") +
  geom_vline(aes(xintercept = median(log_price_4_nights)),
            color ="black",
            linetype ="dashed",
            size = 1)+
  labs(title = "Distribution of Log Prices of Airbnb Apartment in Berlin for 4 nights",
       subtitle = "Data includes only apartments that accomodate at least 2 people",
       x = "Log of Price per 4 nights",
       y = "",
       caption = "Data from: http://insideairbnb.com/get-the-data.html") + 
  theme_bw()

listings %>% 
  ggplot(aes(x = log_price_4_nights)) +
  geom_density(fill = "light blue") +
  geom_vline(aes(xintercept = median(log_price_4_nights)),
            color = "black",
            linetype="dashed", 
            size=1) +
  labs(title = "Distribution of Log Prices of Airbnb Apartment in Berlin for 4 nights",
       subtitle = "Data includes only apartments that accomodate at least 2 people",
       x = "Log of Price per 4 nights",
       y = "",
       caption="Data from: http://insideairbnb.com/get-the-data.html") +
  theme_bw()


```
As we can see, applying a log transformation made the distribution of prices much more like a normal distribution, although there is still a slight right-skew.


```{r}
listings %>% 
  ggplot(aes(x = log_price_4_nights)) +
  geom_boxplot(fill="light blue") +
  labs(title = "Distribution of Log Prices of Airbnb Apartment in Berlin for 4 nights",
       subtitle = "Data includes only apartments that accommodate at least 2 people",
       x="Log of Price per 4 Nights",
       caption="Data from: http://insideairbnb.com/get-the-data.html") + 
  theme_bw()

```
Once we log transform the price variable, there are still several outliers. We will identify these and remove them from our dataset.

```{r, fig.show='hide'}
logprice_outliers <- boxplot(listings$log_price_4_nights)$out

listings<-listings %>% 
  mutate(log_price_4_nights = if_else(log_price_4_nights %in% logprice_outliers,
                                             NA_real_,
                                              log_price_4_nights
                                    )
  )
```


### Property types

Next, we look at the variable `property_type` and use the `count` function to determine how many categories there are, their frequency and their proportions. 

```{r}
listings %>% 
  count(property_type, sort=TRUE) %>% 
  count() #There are 57 categories.
  
property_counts <- listings %>% 
  count(property_type, sort=TRUE) %>%
  mutate(sum_n=sum(n)) %>% 
  slice_max(order_by=n, n=4) %>% 
  mutate(prop=n/sum_n)

property_counts

property_counts %>% 
  summarise(total_prop = sum(prop))
  
```
We can see that there are 57 different property types. The top 4 most common property types are entire rental units (47.2%), private rooms in rental units (35.8%), entire condos (2.5%) & entire serviced apartments (2.0%), which totals 87.6% of the data.

Since the vast majority of the observations in the data are one of the top four or five property types, we create a simplified version of `property_type` variable that has 5 categories: the top four categories and `Other`. 

```{r}
listings <- listings %>%
            mutate(prop_type_simplified = case_when(
            property_type %in% c("Entire rental unit",
                                 "Private room in rental unit", 
                                 "Entire condominium (condo)",
                                 "Entire serviced apartment") ~ property_type, 
                                  TRUE ~ "Other"
                                          )
            )
```

We use the code below to check that `prop_type_simplified` was correctly made.

```{r}
listings %>%
  count(property_type, prop_type_simplified) %>%
  arrange(desc(n))
```

We also make a bar plot.
```{r}

listings %>% 
  ggplot(aes(y=fct_rev(fct_infreq(prop_type_simplified)))) +
  geom_bar(fill = "light blue")+
  labs(
    title="Bar Plot of Simplified Property Type",
    y="Property Type (Simplified)",
    x="Count"
  )+
  NULL


```

Now we can see that the data is much more evenly distributed.


### Bathrooms

The `bathrooms` data is all missing, so we will construct a bathrooms variable using `bathrooms_text`.

```{r, warning = FALSE}

listings<-listings %>% 
  mutate(bathrooms_num=parse_number(bathrooms_text))

```

We will now look at the distribution of the `bathrooms_num` variable.

```{r}

favstats(~bathrooms_num, data=listings)


listings %>% 
  ggplot(aes(x = bathrooms_num)) +
  geom_histogram(fill = "light blue",
                 colour = "grey") +
  labs(title = "Distribution of Bathrooms of Airbnb Apartments in Berlin",
       subtitle = "Data includes only apartments that accommodate at least 2 people",
       x = "Number of bathrooms",
       y = "Count",
       caption="Data from: http://insideairbnb.com/get-the-data.html") + 
  theme_bw()

listings %>% 
  filter(bathrooms_num==0) %>% 
  count()

```

Surprisingly, some apartments have no bathrooms. Perhaps Airbnb classifies toilets and bathrooms separately, or these apartments have access to public toilets in the vicinity.

The data also looks right-skewed. We will make a boxplot to determine whether these extreme points are outliers.

```{r}
listings %>% 
  ggplot(aes(x = bathrooms_num)) +
  geom_boxplot(color="light blue") +
  labs(title = "Boxplot of Bathrooms of Airbnb Apartments in Berlin",
       subtitle = "Data includes only apartments that accommodate at least 2 people",
       x = "Number of bathrooms",
       caption="Data from: http://insideairbnb.com/get-the-data.html") + 
  theme_bw()
```

```{r, fig.show = "hide"}
bathrooms_outliers<-boxplot(listings$bathrooms_num)$out
length(bathrooms_outliers) #There are a large number of 'outliers'. Let's look at these more closely.

bathrooms_outliers %>% table() #Some of these listings have been classified as outliers despite having a reasonable number of bathrooms. However it is unlikely that 2 

#We will only discard apartments with 11 or more beds.
  
listings<-listings %>% 
  mutate(beds = if_else(beds>=11,NA_real_,beds)
  )
```

Some of these listings have been classified as outliers despite having a reasonable number of bathrooms. However, 8 bathrooms is questionable. It is unlikely that 2 people would require an 8 bathroom apartment for a 4-night stay, so for our analysis we will remove this entry.

```{r}
listings<-listings %>% 
  mutate(bathrooms_num = if_else(bathrooms_num>=8,NA_real_,bathrooms_num)
  )
```


### Bed

We will now look at the distribution of the `beds` variable.

```{r}

favstats(~beds, data=listings)


listings %>% 
  ggplot(aes(x = beds)) +
  geom_histogram(fill = "light blue") +
  labs(title = "Distribution of Beds of Airbnb Apartments in Berlin",
       subtitle = "Data includes only apartments that accommodate at least 2 people",
       x = "Number of beds",
       y = "Count",
       caption="Data from: http://insideairbnb.com/get-the-data.html") +
  theme_bw()

```
The data is right-skewed. There is even an apartment in the dataset which claim to have 10 beds. While this is possible, they may also be outliers. We will look at the boxplot.

```{r}

listings %>% 
  ggplot(aes(x = beds)) +
  geom_boxplot(fill="light blue") +
  labs(title = "Boxplot of Beds of Airbnb Apartments in Berlin",
       subtitle = "Data includes only apartments that accommodate at least 2 people",
       x = "Number of beds",
       caption="Data from: http://insideairbnb.com/get-the-data.html") +
  theme_bw()

```
The boxplot confirms that the data is right-skewed. Next we check for and remove some outliers.

```{r, fig.show = "hide"}
beds_outliers<-boxplot(listings$beds)$out
length(beds_outliers) #There are a large number of 'outliers'. Let's look at these more closely.

beds_outliers %>% table() #There are still a large number of apartments with 4-8 beds, so we will not discard these.

#We will only discard apartments with 9 or more beds.
  
listings<-listings %>% 
  mutate(beds = if_else(beds>=9,NA_real_,beds)
  )
```


### Bedrooms

We now perform the same analysis on the `bedrooms` variable.


```{r}

favstats(~bedrooms, data=listings)


listings %>% 
  ggplot(aes(x = bedrooms)) +
  geom_histogram(fill = "light blue") +
  labs(title = "Distribution of Bedrooms of Airbnb Apartments in Berlin",
       subtitle = "Data includes only apartments that accommodate at least 2 people",
       x = "Number of bedrooms",
       y = "Count",
       caption="Data from: http://insideairbnb.com/get-the-data.html") +
  theme_bw()

```

Once again, the data is heavily right-skewed.


```{r}

listings %>% 
  ggplot(aes(x = bedrooms)) +
  geom_boxplot(fill = "light blue") +
  labs(title = "Boxplot of Bedrooms of Airbnb Apartments in Berlin",
       subtitle = "Data includes only apartments that accommodate at least 2 people",
       x = "Number of bedrooms",
       caption="Data from: http://insideairbnb.com/get-the-data.html") +
  theme_bw()

```
This skewness is confirmed in the boxplot.

```{r, fig.show = "hide"}
bedrooms_outliers<-boxplot(listings$bedrooms)$out
length(bedrooms_outliers) #There are a large number of 'outliers'. Let's look at these more closely.

bedrooms_outliers %>% table() #Even apartments with two bedrooms have been categorised as outliers, despite there being a large number in the dataset.

#We will only discard apartments with 10 or more bedrooms.
  
listings<-listings %>% 
  mutate(bedrooms = if_else(bedrooms>=10,NA_real_,bedrooms)
  )
```



### Number of reviews

```{r}

listings %>% 
  ggplot(aes(x = number_of_reviews)) +
  geom_density(fill = "light blue") +
  labs(title = "Distribution of Reviews of Airbnb Apartments in Berlin",
       subtitle = "Data includes only apartments that accommodate at least 2 people",
       x = "Number of reviews",
       caption="Data from: http://insideairbnb.com/get-the-data.html") +
  theme_bw()

```
The distribution is heavily right-skewed. 

Next we will look at the outliers.

```{r}

listings %>% 
  ggplot(aes(x = number_of_reviews)) +
  geom_boxplot(fill = "light blue") +
  labs(title = "Boxplot of Reviews of Airbnb Apartments in Berlin",
       subtitle = "Data includes only apartments that accommodate at least 2 people",
       x = "Number of reviews",
       caption="Data from: http://insideairbnb.com/get-the-data.html") + 
  theme_bw()


```
```{r, fig.show = "hide"}
numreviews_outliers<-boxplot(listings$number_of_reviews)$out
length(numreviews_outliers)

#The boxplot indicates that there are a huge number of outliers.
```


The boxplot indicates that there are a huge number of technical outliers. We will not remove these. Perhaps some apartments have more reviews than others because they are in extremely popular locations, or perhaps they are larger apartments and each guest filled out a review. 


### Reviews per month

Similarly, we will also consider the number of reviews per month.

```{r}
listings %>% 
  ggplot(aes(x = reviews_per_month)) +
  geom_density(fill = "light blue") +
  labs(title = "Distribution of Reviews per month of Airbnb Apartments in Berlin",
       subtitle = "Data includes only apartments that accommodate at least 2 people",
       x = "Reviews per month",
       y = "Density",
       caption="Data from: http://insideairbnb.com/get-the-data.html") +
  theme_bw()
```
As expected, this follows a similar shape to `number_of_reviews`.


```{r}

listings %>% 
  ggplot(aes(x = reviews_per_month)) +
  geom_boxplot(fill = "light blue") +
  labs(title = "Boxplot of Reviews of Airbnb Apartments in Berlin",
       subtitle = "Data includes only apartments that accommodate at least 2 people",
       x = "Reviews per month",
       caption="Data from: http://insideairbnb.com/get-the-data.html") +
  theme_bw()


```
```{r, fig.show = 'hide'}
numreviews_outliers<-boxplot(listings$reviews_per_month)$out
length(numreviews_outliers)

#The boxplot indicates that there are a huge number of outliers.
```

Once again, there are a huge number of technical outliers, which we will not remove.

### Review Scores

There are several measures of review scores in the dataset. To compare these, we start by pivoting the data into long format.

```{r}
reviews_long<-listings %>% 
              select(contains("review_score")) %>% 
              pivot_longer(
                cols=1:ncol(.),
                names_to="review_type",
                values_to="review_scores")
```

Next we look at summary statistics of the variables

```{r}
favstats(review_scores~review_type, data=reviews_long)
```
The variables have quite similar means, however some variables such as review_scores_rating and reviews_scores_cleanliness have higher standard deviations.

We next plot these variables and compare distributions.

```{r}
reviews_long %>% 
  ggplot(aes(x = review_scores, fill=review_type)) +
  geom_density() +
  facet_wrap(~review_type, ncol=3)+
  labs(title = "Distribution of Review Scores of Airbnb Apartments in Berlin",
       subtitle = "Data includes only apartments that accommodate at least 2 people",
       x = "Review Scores",
       y = "Density",
       caption="Data from: http://insideairbnb.com/get-the-data.html") +
  theme_bw() +
  theme(legend.position="none")

```
At first glance, the variables look fairly similar, with most reviews falling in the range 4-5.

```{r}
listings %>% 
  select(contains("review_score")) %>%
  ggpairs()

```

The correlation plot further confirms that there is some degree of correlation between the variables.


### Neighbourhoods

Next, we want to explore the 3 variables related to neighborhoods `neighbourhood`, `neighbourhood_cleansed`, and `neighbourhood_group_cleansed` to understand which variable describes better the actual neighbourhoods in Berlin. 

```{r}
listings %>% 
  select("neighbourhood","neighbourhood_cleansed","neighbourhood_group_cleansed") %>% 
  head()
```
The 'neighbourhood_group_cleansed' variable is the most appropriate to evaluate further, as it groups the correct unique neighbourhoods in Berlin. 

Next, we want to see how many different neighborhoods we have in the dataset and how frequently they appear. We will be looking at the 'neighbourhood_group_cleansed' variable.

```{r}

listings %>%
  count(neighbourhood_group_cleansed, sort=TRUE) %>% 
  count() #There are 12 categories.
  
neighbourhood_counts <- listings %>% 
  count(neighbourhood_group_cleansed, sort=TRUE) %>%
  mutate(sum_n=sum(n)) %>% 
  slice_max(order_by=n, n=6) %>% 
  mutate(prop=n/sum_n)

neighbourhood_counts

neighbourhood_counts %>% 
  summarise(total_prop = sum(prop))
  
  
```

We have a total of 12 neighbourhoods, however 6 of them represent 88.4% of all the observations. From our knowledge of Berlin, these 6 neighbourhoods are indeed the most popular ones while the others are more on the outskirts of the city, therefore, we will group the other 6 ones into the "Other" category.

```{r}
listings <- listings %>%
    mutate(neighbourhood_simplified = case_when(
    neighbourhood_group_cleansed %in% c("Friedrichshain-Kreuzberg",
                         "Mitte", 
                         "Pankow",
                         "Neukölln",
                         "Charlottenburg-Wilm.",
                         "Tempelhof - Schöneberg") ~ neighbourhood_group_cleansed, 
                        TRUE ~ "Other"
  ))
```

We can check our output below.

```{r}
listings %>%
  count(neighbourhood_group_cleansed, neighbourhood_simplified) %>%
  arrange(desc(n))
```
Now let's look at this distribution of neighbourhoods visually.

```{r}
listings %>% 
  ggplot(aes(y = fct_rev(fct_infreq(neighbourhood_simplified)))) +
  geom_bar(fill = "light blue") +
  labs(title = "Distribution of neighbourhoods of Airbnb Apartments in Berlin",
       subtitle = "Data includes only apartments that accommodate at least 2 people",
       y = "Neighbourhood",
       x = "Count",
       caption="Data from: http://insideairbnb.com/get-the-data.html") + 
  theme_bw()
```
### Superhost

In our analysis, we will also consider whether being a 'superhost' is an important factor in determining the price of a 4-night stay. Let's look at the proportions of listings which fall into each category.

```{r}
listings %>% 
  count(host_is_superhost) %>% 
  mutate(total=sum(n),
         prop=n/total)
```
Only about 17% of the listings are made by 'superhosts'.

### Availability 30

Finally, we consider the `availability_30` variable. We expect this variable to range from 0-30, corresponding to the number of the days the apartment is available in the next 30 days. This variable may be a good predictor of price as it indicates the relative demand for an apartment - rooms which have low availability over the next 30 days are likely to have higher demand and can therefore charge higher prices.
```{r}
favstats(~availability_30, data=listings)

```
An examination of the summary statistics shows that the minimum is 0 and the maximum is 30, as we would expect. Interestingly, the mean is approximately 4, indicating that apartments are typically booked out for the majority of the next 30 day period.

We also consider the distribution of availability.

```{r}
listings %>% 
  ggplot(aes(x = availability_30)) +
  geom_density(fill="light blue") +
  labs(title = "Distribution of 30-Day Availability of Airbnb Apartments in Berlin",
       subtitle = "Data includes only apartments that accommodate at least 2 people",
       x = "Availability in next 30 days",
       y="Density",
       caption="Data from: http://insideairbnb.com/get-the-data.html") +
  theme_bw()

```
The distribution has a strong positive skew.

## Correlation of selected continous variables with "log_price_4_nights"

> Next, we will look at the correlations of continuous variables. As we want to evaluate multiple variables, first we will look at those not related to the reviews. 

```{r}
listings %>% 
  ggpairs(columns = c(
                      "log_price_4_nights",
                      "bedrooms", 
                      "beds",
                      "accommodates",
                      "host_response_rate",
                      "host_listings_count",
                      "availability_30",
                      "bathrooms_num",
                      "calculated_host_listings_count"),
          upper = list(continuous = "cor"),
          lower = list(continuous = "points"))
```

> We observe that "log_price_4_nights" has the highest positive correlation with the variables: "bedrooms" (0.428), "beds" (0.374), "accommodate" (0.506), "availability_30" (0.354), "bathrooms_num" (0.137), and "calculated_host_listings_count" (0.207). 

> In the next step, we will evaluate correlations between the "log_price_4_nights" and reviews-related numeric variables:

```{r}
listings %>% 
  ggpairs(columns = c(
                      "log_price_4_nights",
                      "number_of_reviews",
                      "review_scores_rating",
                      "number_of_reviews_l30d",
                      "reviews_per_month",
                      "review_scores_value",
                      "review_scores_cleanliness",
                      "review_scores_location",
                      "review_scores_checkin",
                      "review_scores_accuracy"),
           upper = list(continuous = "cor"),
          lower = list(continuous = "points"))
```

> We observe that "log_price_4_nights" has the highest correlation with the variables: "number of reviews" (0.094), "review_scores_location" (0.078), "number_of_reviews_l30d" (0.089), "reviews_per_month" (0.141), "review_scores_value" (-0.100). However, compared to the numerical variables evaluated above,reviews-related variables seem to have lower correlations with the price.

## Correlation of selected categorical variables with "log_price_4_nights"

> Next, we will separately evaluate correlations of categorical variables.
  
```{r}
listings %>% 
  ggpairs(columns = c("log_price_4_nights",
                      "prop_type_simplified",
                      "room_type",
                      "neighbourhood_simplified"
                      ))
```
  
> Looking at the boxplots, we notice that that "property_type_simplified" and "room_type" seem to have the highest correlation with the "log_price_4_nights". 
    
@group4 Optional extra fun
- What are the correlations between variables? Does each scatterplot support a linear relationship between variables? Do any of the correlations appear to be conditional on the value of a categorical variable?


## Data wrangling


## USE LOGARITHM OF PRICE BECAUSE IT IS HEAVILY SKEWED. WE ALSO NEED TO REMOVE EXTREME VALUES, NEGATIVE PRICES. IF YOU TRANSFORM THE VARIABLES (E.G. BY USING FACTOR LEVELS). EXPECT YOUR ADJUSTED R-SQUARED TO BE BETWEEN 0.1-0.5. IN THE PRESENTATION, DON'T PUT SCREENSHOTS OF THE REGRESSION OUTPUT. SHOW VISUALISATIONS. SHOW CONFIDENCE INTERVALS. WHEN YOU BUILD YOUR MODEL, SPLIT IT INTO TRAINING AND VALIDATION. IN THE SLIDES, PEOPLE ONLY CARE ABOUT THE FINAL MODEL.SHOW THE RESULTS; NO ONE CARES ABOUT THE PROBLEM. DON'T SHOW US A HISTOGRAM - USE MORE ADVANCED GGPLOTS. FEEL FREE TO CRITICIZE THE MODEL AND GIVE ADVICE ABOUT WHAT DATA WOULD BE HELPFUL TO IMPROVE THE MODEL.THE ECONOMIST THEME IS HELPFUL. YOU CAN USE KABLE EXTREME TO MAKE TABLES LOOK NICE.YOU MIGHT WANT TO SHOW CONFIDENCE INTERVALS FOR DIFFERENT PROPERTY TYPES

##USEFUL VARIABLES: review score, number of reviews, superhost, accomodate, property type, room type, cancellation policy, bathrooms, bedrooms, neighbourhood.


##Use at least 960 data points for training. Use at least 40 observations per variable.
##Weekly price might fit better than daily price.

DO NOT CHANGE NA TO 0.
YOU MIGHT WANT TO CREATE A VARIABLE FOR GOOD NEIGHBOURHOODS AND BAD NEIGHBOURHOODS.
DON'T INCLUDE HOST_ID AND OTHER IDENTIFIERS IN THE MODEL.

Feel free to use interaction terms.



# Mapping 

Visualisations of feature distributions and their relations are key to understanding a data set, and they can open up new lines of exploration. We will use the following code to show a map of your city, and overlay all Airbnb coordinates to get an overview of the spatial distribution of properties. For this visualization, we use the `leaflet` package, which includes a variety of tools for interactive maps and allows for easy zoom in-out, clicking on a point to get the actual listing, etc.

```{r, out.width = '80%'}

leaflet(data = listings) %>% 
  addProviderTiles("OpenStreetMap.Mapnik") %>% 
  addCircleMarkers(lng = ~longitude, 
                   lat = ~latitude, 
                   radius = 1, 
                   fillColor = "light blue", 
                   fillOpacity = 0.4, 
                   popup = ~listing_url,
                   label = ~property_type)
```

  # Regression Analysis

For the target variable $Y$, we will use the cost for two people to stay at an Airbnb location for four (4) nights. 

We have already created a new variable called `price_4_nights` that uses `price`, and `accomodates` to calculate the total cost for two people to stay at the Airbnb property for 4 nights. This is the variable $Y$ we want to explain.

Use histograms or density plots to examine the distributions of `price_4_nights` and `log(price_4_nights)`. Which variable should you use for the regression model? Why?

Fit a regression model called `model1` with the following explanatory variables: `prop_type_simplified`, `number_of_reviews`, and `review_scores_rating`. 

- Interpret the coefficient `review_scores_rating` in terms of `price_4_nights`.
- Interpret the coefficient of `prop_type_simplified` in terms of `price_4_nights`.

We want to determine if `room_type` is a significant predictor of the cost for 4 nights, given everything else in the model. Fit a regression model called model2 that includes all of the explananatory variables in `model1` plus `room_type`. 


# Creating a Control Sample

First, we need to divide our data into a test and training subset. This will allows us to test the predictive power of our model using an out-of-sample test. Since we are interested in renting for two, we will exclude any properties that accommodate less than 2 people, as well as any properties that have a maximum stay requirement of less than 4 nights.

```{r}

# We remove properties that accommodate less than 2 people or have a maximum night restriction of less than 4 nights
listings <- listings %>% 
  filter(accommodates >= 2,
          maximum_nights >= 4)


#splitting data into training and test sets

set.seed(1234) #setting a random seed to always get the same split 

train_test_split <- initial_split(listings, prop = 0.80) #taking an 80-20 approach (for all the consultants out there)
listings_train <- training(train_test_split)
listings_test <- testing(train_test_split)

```

Now our test dataset has 3658 observations and training data has 14630 observations with 74 variables each.

We will set our test data aside and do our analysis using only the training data

```{r}

# Model 1 - Property Type, Number of Reviews, Review Scores
model1 <- lm(log_price_4_nights ~ prop_type_simplified + 
                                  number_of_reviews + 
                                  review_scores_rating, 
             data = listings_train)

# Since prop_type_simplified is a categorical variable with 5 possible options, the model only shows 4 coefficients. The fifth one (Entire Condo), is incorporated in the intercept

# We construct a table with the regression output
huxtable:: huxreg(list("Model1" = model1), 
                  statistics = c('#observations' = 'nobs', 
                                'R squared' = 'r.squared', 
                                'Adj. R Squared' = 'adj.r.squared', 
                                'Residual SE' = 'sigma',
                                'P-Value' = 'p.value'))

model1 %>% 
broom::tidy(conf.int=TRUE)

model1 %>% 
   broom::glance()
```
# @GROUP4: UPDATE COEFFICIENTS and statistical significance!!
Overall, Model 1 has p-value of 0, which means it is statistically significant. Furthermore, according to R squared, our model explains 36.88% of the variation in log_price_4_nights with an adjusted R squared of 36.8%. Additionally, all the variables used are significant at the 95% confidence level. 

Since the variable property_type is categorical  with 5 possible options, the model only shows 4 coefficients. The fifth one (Entire Condo), is incorporated in the intercept. This means the coefficients of the the other four are in relation to the fifth one. To interpret the coefficients for a log-transformed dependent variable, we are going to take the exponent of the coefficient. In this case, entire rental units are on average 12.8% cheaper than entire condos for 4 nights. Similarly, entire serviced apartments are 58.6% more expensive than entire condos, whereas private rooms in rental units are 57.0% cheaper than entire condos. Finally, other types of properties are on average 19.9% cheaper than entire condos.

Moreover, we see that one unit change in review_scores_rating corresponds to 3.6% higher price per 4 nights. 

Finally, every additional review corresponds to 0.05% higher price per 4 nights. This is quite small but we also have to consider that the unit change in number_of_reviews is a single review. The maximum number of reviews on a property is 655 and we can see that if the unit change was 100, for every 100 additional reviews, the price per 4 nights would increase by 5.7%, which looks much more substantial.

```{r}

# Model 2 - Model 1 + Room Type

model2 <- lm(log_price_4_nights ~  prop_type_simplified + 
                                   number_of_reviews + 
                                   review_scores_rating +
                                   room_type,
             data = listings_train)

# Since room_type is a categorical variable with 4 possible options, the model only shows 3 coefficients. The fourth option (Entire Home/ Apartment) is incorporated in the intercept

# We construct a table with the regression output
huxtable:: huxreg(list("Model2" = model2), 
                  statistics = c('#observations' = 'nobs', 
                                'R squared' = 'r.squared', 
                                'Adj. R Squared' = 'adj.r.squared', 
                                'Residual SE' = 'sigma',
                                'P-Value' = 'p.value'))

model2 %>% 
broom::tidy(conf.int=TRUE)

model2 %>% 
   broom::glance()

```
# @GROUP4: UPDATE COEFFICIENTS and statistical significance!!
Model 2 is also statistically significant, with a p-value of 0. Additionally, it explains 41.2% of the variation in log_price_4_nights, which is higher than Model 1. All variables are statistically significant. To compare the two models, however, we need to look at the adjusted r-squared, since Model 2 includes one additional variable. Model 2 has an adjusted r-squared of 41.11% compared to 36.83% for Model 1. We can conclude that Model 2 has a higher predictive power than Model 1.

Since room_type is a categorical variable with 4 possible options, the model only shows 3 coefficients. The fourth option (Entire Home/ Apartment) is incorporated in the intercept. To interpret the coefficients for a log-transformed dependent variable, we are going to take the exponent of the coefficient. This means that hotel rooms, for example, are on average 55.28% more expensive than entire home/apartments. Similarly, private rooms 41.4% cheaper, whereas shared rooms are 59.59% cheaper than entire homes.

Now, before we continue with additional model specifications, we are going to perform a VIF test to echeck for multicollinearity:
```{r}
autoplot(model2)

car::vif(model2)
```
We  can see that room_type and property_type_simplified have VIF scores of more than 10, which means there is potential multicollinearity problem in our model. Before we proceed with the analysis, we will test which variable has a higher predictive power and remove the other:

```{r}

model2_property <- lm(log_price_4_nights ~  prop_type_simplified + 
                                           number_of_reviews + 
                                           review_scores_rating,
                     data = listings_train)
model2_room <- lm(log_price_4_nights ~  number_of_reviews + 
                                       review_scores_rating +
                                       room_type,
                     data = listings_train)

huxtable:: huxreg(list("Property Type Model" = model2_property, 
                       "Room Type Model" = model2_room), 
                  statistics = c('#observations' = 'nobs', 
                                'R squared' = 'r.squared', 
                                'Adj. R Squared' = 'adj.r.squared', 
                                'Residual SE' = 'sigma')
                  ) %>% 
  set_caption('Comparison of Property Type vs. Room Type Models')

model2 %>% 
broom::tidy(conf.int=TRUE)

model2 %>% 
   broom::glance()
```

As we can see from the table, the model that includes room types instead of property types has both a higher r-squared and adj. r-squared. To avoid multicollinearity, we will continue using that model and will discard property type.

```{r}
car::vif(model2_room)

```


## Further variables/questions to explore on our own

Our dataset has many more variables, so we attempt to extend our analysis by adding a number of variables, which could prove useful in increasing the explanatory power of our model.

1. Are the number of `bathrooms`, `bedrooms`, `beds`, or size of the house (`accomodates`) significant predictors of `price_4_nights`? Or might these be co-linear variables?
2. Do superhosts `(host_is_superhost`) command a pricing premium, after controlling for other variables?
3. Some hosts allow you to immediately book their listing (`instant_bookable == TRUE`), while a non-trivial proportion don't. After controlling for other variables, is `instant_bookable` a significant predictor of `price_4_nights`?
4. For all cities, there are 3 variables that relate to neighbourhoods: `neighbourhood`, `neighbourhood_cleansed`, and `neighbourhood_group_cleansed`. There are typically more than 20 neighbourhoods in each city, and it wouldn't make sense to include them all in your model. Use your city knowledge, or ask someone with city knowledge, and see whether you can group neighbourhoods together so the majority of listings falls in fewer (5-6 max) geographical areas. You would thus need to create a new categorical variabale `neighbourhood_simplified` and determine whether location is a predictor of `price_4_nights`
5. What is the effect of `avalability_30` or `reviews_per_month` on `price_4_nights`, after we control for other variables?

```{r}

# Model 3 - Adding property-specific characteristics to Model_Rooms. We exclude `bathrooms` because there is not data for any of the observations 

model3 <- lm(log_price_4_nights ~ number_of_reviews + 
                                  review_scores_rating +
                                  room_type +
                                  bedrooms +
                                  bathrooms_num+
                                  beds +
                                  accommodates,
                     data = listings_train)

# We construct a table with the regression output
huxtable:: huxreg(list("Amenities Model" = model3), 
                  statistics = c('#observations' = 'nobs', 
                                'R squared' = 'r.squared', 
                                'Adj. R Squared' = 'adj.r.squared', 
                                'Residual SE' = 'sigma',
                                'P-Value' = 'p.value')
                  )
model3 %>% 
broom::tidy(conf.int=TRUE)

model3 %>% 
   broom::glance()


```

# @GROUP4: UPDATE COEFFICIENTS and statistical significance!!
We see from the table that Model 3 is statistically significant and adding information about property amenities like number of bedrooms and how many people a property can accommodate greatly increases the predictive power of the model. We see an r-squared of 50.9% and an adjusted r-squared of 50.9%. Looking at the statistical significance of the explanatory variables, we find that they are all statistically significant at a least a 95% confidence level.

However, we need be weary of potential multicollinearity. Logivally, it could expected that bedrooms, beds, and 'accommodates' would be highly correlated. Therefore, we test for multicollinearity by running af VIF-test.

```{r}

# We test for multicollinearity
car::vif(model3)

```

Although the results don't suggest there is multicollinearity problem with Model 3, we will attempt to remove `accommodates` to see the effect:

```{r}

# Model 4 A - Model 3 without `accommodates`

model4_a <- lm(log_price_4_nights ~ number_of_reviews + 
                                  review_scores_rating +
                                  room_type +
                                  bedrooms +
                                  bathrooms_num+
                                  beds,
                     data = listings_train)

# We construct a table with the regression output
huxtable:: huxreg(list("No-Accommodates Model" = model4_a), 
                  statistics = c('#observations' = 'nobs', 
                                'R squared' = 'r.squared', 
                                'Adj. R Squared' = 'adj.r.squared', 
                                'Residual SE' = 'sigma',
                                'P-Value' = 'p.value')
                  )
car::vif(model4_a)
```

The model and all the descriptive statistics prove statistically significant. The predictive power of our model has decreased, however, from 40.5% adjusted r-squared to 48.4%. We will now try removing `beds` and running the regression with `accommodates` instead:

```{r}

# Model 4 B - Model 3 without `beds`

model4_b <- lm(log_price_4_nights ~ number_of_reviews + 
                                  review_scores_rating +
                                  room_type +
                                  bedrooms +
                                  bathrooms_num+
                                  accommodates,
                     data = listings_train)

# We construct a table with the regression output
huxtable:: huxreg(list("No Beds Model" = model4_b), 
                  statistics = c('#observations' = 'nobs', 
                                'R squared' = 'r.squared', 
                                'Adj. R Squared' = 'adj.r.squared', 
                                'Residual SE' = 'sigma',
                                'P-Value' = 'p.value')
                  )
car::vif(model4_b)
```

The predictive power of Model 4_b is higher than Model 4_a and all variables are statistically significant at the 95% confidence level. Comparing with model 3, we see that we arrive at a similar explanatory power of 50.9 % with a reduced number of explanatory variables. Furthermore, we see that the VIF-analysis returns a lower level of multicollinearity. Thus, we move forward with model 4_b, renaming it to model4.

```{r}
# Model 5 - Model 4 + host_is_superhost


# Renaming Model 4 B to Model 4
model4 <- model4_b

model5 <- lm(log_price_4_nights ~ number_of_reviews + 
                                  review_scores_rating +
                                  room_type +
                                  bedrooms +
                                  bathrooms_num+
                                  accommodates +
                                  host_is_superhost,
                     data = listings_train)

# We construct a table with the regression output
huxtable:: huxreg(list("Superhost Model" = model5), 
                  statistics = c('#observations' = 'nobs', 
                                'R squared' = 'r.squared', 
                                'Adj. R Squared' = 'adj.r.squared', 
                                'Residual SE' = 'sigma',
                                'P-Value' = 'p.value')
                  )
car::vif(model5)



```
Model 5 is statistically significant, with a p-value of 0. Additionally, Model 5 has a slightly better predictive power than Model 4, although the difference in adj. r-squared is very small (0.3%). The new variable we added, host_is_superhost, is statistically significant, but now the number_of_reviews is not. One potential explanation is that the the number of reviews has different effect on the price depending on whether the host is a superhost or not. To deal with this, we will create an interaction term between `host_is_superhost` and `number_of_reviews`:

```{r}

model5_inter <- lm(log_price_4_nights ~ number_of_reviews + 
                                  review_scores_rating +
                                  room_type +
                                  bedrooms +
                                  bathrooms_num+
                                  accommodates+
                                  host_is_superhost+
                                  host_is_superhost*number_of_reviews,
                     data = listings_train)

# We construct a table with the regression output
huxtable:: huxreg(list("Superhost Model + Interaction" = model5_inter), 
                  statistics = c('#observations' = 'nobs', 
                                'R squared' = 'r.squared', 
                                'Adj. R Squared' = 'adj.r.squared', 
                                'Residual SE' = 'sigma',
                                'P-Value' = 'p.value')
                  )
car::vif(model5_inter)

model5_inter %>% 
broom::tidy(conf.int=TRUE)

```

First, we see that including the interaction term made number_of_reviews statistically significant again but the new model has a the same predictive power as the old one (51.2% adj. R squared). The way to interpret the new coefficient is as follows: when the host is not superhost, each additional review corresponds to approximately 0.03% higher price. When the host is a superhost, each additional review corresponds to 0.03% lower price. Additionally, being a superhost corresponds to 11.1% higher price when controlling for the number of reviews. Thus, all variables are statistically significant and the predictive power of our model is the highest we have achieved so far. However, intuitively, it does not make sense that an increase in number of reviews of superhosts decreases price.
# THIS DOESN'T MAKE SENSE, PLEASE DOUBLE CHECK

<!-- We now see that review score is nolonger significant. One explanation for this could be that being a superhost is directly related to the score of a host. We will try to solve this problem by removing review_scores_rating -->


<!-- ```{r} -->
<!-- model6 <- lm(log_price_4_nights ~ number_of_reviews + -->
<!--                                   room_type + -->
<!--                                   bedrooms + -->
<!--                                   beds+ -->
<!--                                   host_is_superhost+ -->
<!--                                   host_is_superhost*number_of_reviews, -->
<!--                      data = listings_train) -->

<!-- # We construct a table with the regression output -->
<!-- huxtable:: huxreg(list("Superhost Model + Interaction - Reviews Score" = model6), -->
<!--                   statistics = c('#observations' = 'nobs', -->
<!--                                 'R squared' = 'r.squared', -->
<!--                                 'Adj. R Squared' = 'adj.r.squared', -->
<!--                                 'Residual SE' = 'sigma', -->
<!--                                 'P-Value' = 'p.value') -->
<!--                   ) -->
<!-- car::vif(model6) -->

<!-- model6 %>% -->
<!-- broom::tidy(conf.int=TRUE) -->
<!-- ``` -->

<!-- Modle 6 has much lower predictive power than our previous model. Additionally, number_of_reviews is no longer statistically significant. The variable host_is_superhost is statistically significant and has a large effect on the log_price_4_nights. The probelm is that host_is_superhost is directly related to the number of reviews a host has and how high their score is. To determine weather we should proceed with host_is_superhost or review_score_rating and number_of_reviews, we will create another model and compare it to Model 4: -->

<!-- ```{r} -->
<!-- model7 <- lm(log_price_4_nights ~ room_type + -->
<!--                                   bedrooms + -->
<!--                                   accommodates+ -->
<!--                                   host_is_superhost, -->
<!--                      data = listings_train) -->

<!-- # We construct a table with the regression output -->
<!-- huxtable:: huxreg(list("Superhost Model Only" = model7),  -->
<!--                   statistics = c('#observations' = 'nobs',  -->
<!--                                 'R squared' = 'r.squared',  -->
<!--                                 'Adj. R Squared' = 'adj.r.squared',  -->
<!--                                 'Residual SE' = 'sigma', -->
<!--                                 'P-Value' = 'p.value') -->
<!--                   ) -->
<!-- car::vif(model7) -->

<!-- model7 %>%  -->
<!-- broom::tidy(conf.int=TRUE) -->


<!-- ``` -->


<!-- Although all variables in Model 7 are statistically significant, the predictive power of the model is lower than Model 4. This means that host_is_superhost is less powerful than the combination of review_score_rating and number_of_reviews and we will not include it in our model. We will continue with our analysis using model specification 4. -->

Next, we would like to include the variable `neighbourhood_simplified` with the following code:

```{r}

# Renaming Model 5 Interaction to Model 5
model5 <- model5_inter

model6 <- lm(log_price_4_nights ~ number_of_reviews + 
                                  review_scores_rating +
                                  room_type +
                                  bedrooms +
                                  bathrooms_num+
                                  accommodates +
                                  host_is_superhost +
                                  neighbourhood_simplified,
                     data = listings_train)

# We construct a table with the regression output
huxtable:: huxreg(list("Neighbourhoods" = model6), 
                  statistics = c('#observations' = 'nobs', 
                                'R squared' = 'r.squared', 
                                'Adj. R Squared' = 'adj.r.squared', 
                                'Residual SE' = 'sigma',
                                'P-Value' = 'p.value')
                  )
car::vif(model6)

model6 %>% 
broom::tidy(conf.int=TRUE)



```

Model 6 is statistically significant and has the highest adjusted r-squared so far at 52.6%. Additionally, 52.7% of the variation in log_price_4_nights can be exmplained by Model 6. The variable `neighbourhood_simplified` has 6 possible values: Friedrichshain-Kreuzberg, Mitte, Pankow, Neukölln, Charlottenburg-Wilm, Tempelhof - Schöneberg, and Other. Since `neighbourhood_simplified` is a categorical variable, only 5 possible outcomes have their own coefficient, and the last one,Charlottenburg-Wilm , is incorporated into the intercept. Although not all neighbourhoods have a statistically significant effect on log price, properties in Mitte are on average 6.5% more expensive, where as properties in Neukolln, Pankow, and Tempelhof are 11.7%, 0.3%, and 5.8% cheaper than Charlottenburg-Wilm respectively.

Since some neighbourhoods have a significant effect on log price and this model has the highest predictive power so far, we will proceed with Model 6.

Next, we would like to add `avalability_30` or `reviews_per_month` to our analysis with the following code:

```{r}

model7 <- lm(log_price_4_nights ~ number_of_reviews + 
                                  review_scores_rating +
                                  room_type +
                                  bedrooms +
                                  bathrooms_num +
                                  accommodates+
                                  host_is_superhost+
                                  host_is_superhost*number_of_reviews+
                                  neighbourhood_simplified+
                                  availability_30+
                                  reviews_per_month,
                     data = listings_train)

# We construct a table with the regression output
huxtable:: huxreg(list("Reviews Per Month" = model7), 
                  statistics = c('#observations' = 'nobs', 
                                'R squared' = 'r.squared', 
                                'Adj. R Squared' = 'adj.r.squared', 
                                'Residual SE' = 'sigma',
                                'P-Value' = 'p.value')
                  )
car::vif(model7)

model7 %>% 
broom::tidy(conf.int=TRUE)

```

Model 7 is statistically significant and has the highest predictive power so far at 59.2% adjusted r-squared. Additionally, 59.3% of the variation in log_price_4_nights is explained by the model. Both `availability_30` and `reviews_per_month` are statistically significant. Look at the VIF test, we see that there is no problem with multicollinearity, which would suggest we should not remove reviews_per_month. Looking at `number_of_reviews`, we see that while the variable's coefficient was insignificant in Model 6, it is significant now. In other words, controlling for the reviews per month a property receives makes the number of reviews a property has significant. This means we should *include* `reviews_per_month` in our model, despite it being *not statistically significant*.

Following this, we would like to increase the predictive power of our model as much as possible. To do that we will start testing the predictive power of the following variables one by one:

- host_response_rate 
- host_listings_count
- maximum_nights
- number_of_reviews_ltm
- review_scores_accuracy
- review_scores_cleanliness
- review_scores_checkin
- review_scores_communication
- review_scores_location
- review_scores_value
- longitude
- latitude
- number_of_reviews_l30


```{r}
model8 <- lm(log_price_4_nights ~ number_of_reviews + 
                                  review_scores_rating +
                                  room_type +
                                  bedrooms +
                                  bathrooms_num+
                                  accommodates +
                                  host_is_superhost +
                                  neighbourhood_simplified +
                                  availability_30 +
                                  reviews_per_month +
                                  review_scores_value +
                                  review_scores_cleanliness +
                                  review_scores_location +
                                  review_scores_checkin +
                                  review_scores_accuracy +
                                  latitude +
                                  longitude +
                                  number_of_reviews_l30d +
                                  calculated_host_listings_count,
                     data = listings_train)

# We construct a table with the regression output
huxtable:: huxreg(list("model8" = model8), 
                  statistics = c('#observations' = 'nobs', 
                                'R squared' = 'r.squared', 
                                'Adj. R Squared' = 'adj.r.squared', 
                                'Residual SE' = 'sigma',
                                'P-Value' = 'p.value')
                  )
car::vif(model8)


```

```{r}
model9 <- lm(log_price_4_nights ~ 
                                  room_type +
                                  accommodates +
                                  bathrooms_num+
                                  host_is_superhost +
                                  availability_30 +
                                  review_scores_value +
                                  review_scores_cleanliness +
                                  review_scores_location,
                     data = listings_train)

# We construct a table with the regression output
huxtable:: huxreg(list("model9" = model9), 
                  statistics = c('#observations' = 'nobs', 
                                'R squared' = 'r.squared', 
                                'Adj. R Squared' = 'adj.r.squared', 
                                'Residual SE' = 'sigma',
                                'P-Value' = 'p.value')
                  )
car::vif(model9)
```
In our model 8, we included 13 variables, many with a number of subvariables. The explanatory power was 62%, which although great, made the model complicated. In model 9, we observe that when retaining significant and relevant variables, with only 7 variables, we are able to reduce number of explanatory variables by 6 variables and only drop explanatory power by 4.4 %, which leads to a less complicated and superior model from a complexity vs. explanatory power-tradeoff. Comparing model 9

Now, we check the residuals of our two models.

```{r}
autoplot(model1)
autoplot(model2)
autoplot(model3)
autoplot(model4)
autoplot(model5)
autoplot(model6)
autoplot(model7)
autoplot(model8)
autoplot(model9)

huxtable:: huxreg(list(model1,model2, model3, model4, model5, model6, model7, model8, model9), 
                  statistics = c('#observations' = 'nobs', 
                                'R squared' = 'r.squared', 
                                'Adj. R Squared' = 'adj.r.squared', 
                                'Residual SE' = 'sigma',
                                'P-Value' = 'p.value')
                  )

```

@GROUP4 : WHAT DOES THIS MEAN??

## Diagnostics, collinearity, summary tables

As you keep building your models, it makes sense to:

1. Check the residuals, using `autoplot(model_x)`
1. As you start building models with more explanatory variables, make sure you use `car::vif(model_x)`` to calculate the **Variance Inflation Factor (VIF)** for your predictors and determine whether you have colinear variables. A general guideline is that a VIF larger than 5 or 10 is large, and your model may suffer from collinearity. Remove the variable in question and run your model again without it.

1. Create a summary table, using `huxtable` (https://mfa2022.netlify.app/example/modelling_side_by_side_tables/) that shows which models you worked on, which predictors are significant, the adjusted $R^2$, and the Residual Standard Error.
1. Finally, you must use the best model you came up with for prediction. Suppose you are planning to visit the city you have been assigned to over reading week, and you want to stay in an Airbnb. Find Airbnb's in your destination city that are apartments with a private room, have at least 10 reviews, and an average rating of at least 90. Use your best model to predict the total cost to stay at this Airbnb for 4 nights. Include the appropriate 95% interval with your prediction. Report the point prediction and interval in terms of `price_4_nights`. 
  - if you used a log(price_4_nights) model, make sure you anti-log to convert the value in $. You can read more about [hot to interpret a regression model when some variables are log transformed here](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faqhow-do-i-interpret-a-regression-model-when-some-variables-are-log-transformed/)


# Deliverables


- By midnight on Monday 18 Oct 2021, you must upload on Canvas a short presentation (max 4-5 slides) with your findings, as some groups will be asked to present in class. You should present your Exploratory Data Analysis, as well as your best model. In addition, you must upload on Canvas your final report, written  using R Markdown to introduce, frame, and describe your story and findings. You should include the following in the memo:

1. Executive Summary: Based on your best model, indicate the factors that influence `price_4_nights`.
This should be written for an intelligent but non-technical audience. All
other sections can include technical writing.
2. Data Exploration and Feature Selection: Present key elements of the data, including tables and
graphs that help the reader understand the important variables in the dataset. Describe how the
data was cleaned and prepared, including feature selection, transformations, interactions, and
other approaches you considered.
3. Model Selection and Validation: Describe the model fitting and validation process used. State
the model you selected and why they are preferable to other choices.
4. Findings and Recommendations: Interpret the results of the selected model and discuss
additional steps that might improve the analysis
  
  

Remember to follow R Markdown etiquette rules and style; don't have the Rmd output extraneous messages or warnings, include summary tables in nice tables (use `kableExtra`), and remove any placeholder texts from past Rmd templates; in other words, (i.e. I don't want to see stuff I wrote in your final report.)
  
  
# Rubric

Your work will be assessed on a rubric which you can find here


```{r rubric, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "rubric.png"), error = FALSE)
```


# Acknowledgements

- The data for this project is from [insideairbnb.com](insideairbnb.com)